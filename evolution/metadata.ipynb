{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ed8c94-fe42-46ba-915a-de9a5768358f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95 (+/- 0.04) [Logistic Regression]\n",
      "Accuracy: 0.94 (+/- 0.04) [Random Forest]\n",
      "Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.04) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "\n",
    "# loading iris dataset \n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "# Voting Classifier with hard voting \n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],voting='hard')\n",
    "\n",
    "# Ensemble of Models \n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    \n",
    "#estimator = [] \n",
    "#estimator.append(('LR',LogisticRegression(solver ='lbfgs',multi_class ='multinomial',max_iter = 200))) \n",
    "#estimator.append(('SVC', SVC(gamma ='auto', probability = True))) \n",
    "# estimator.append(('DTC', DecisionTreeClassifier())) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd241f32-f1d7-47c6-8be3-ad66dddb83df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3128285/2738165774.py\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# calling the model and compile it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mseq_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     seq_model.compile(\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_model' is not defined"
     ]
    }
   ],
   "source": [
    "# More Model ensembling techniques now #\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# Loading some example data\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "\n",
    "y = iris.target\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier(max_depth=4)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf3 = SVC(kernel='rbf', probability=True)\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)], voting='soft', weights=[2, 1, 2])\n",
    "\n",
    "clf1 = clf1.fit(X, y)\n",
    "clf2 = clf2.fit(X, y)\n",
    "clf3 = clf3.fit(X, y)\n",
    "eclf = eclf.fit(X, y)\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Deicision Tree', 'K-Nearest Neighbour', 'SVC', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7269cc0d-a529-4233-8375-0c32fe023f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "(100, 2)\n",
      "(100,)\n",
      "Test Set\n",
      "(50, 2)\n",
      "(50,)\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 10) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3128285/4189055219.py\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# run the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     model.fit(X[train], y[train],\n\u001b[0m\u001b[1;32m     63\u001b[0m               batch_size=128, epochs=2, validation_data=(X[test], y[test]))\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 10) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Now we want to analyse the models using tfma \n",
    "# This setup was tested with TF 2.10 and TFMA 0.41 (using colab), but it should\n",
    "# also work with the latest release.\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Confirm that we're using Python 3\n",
    "assert sys.version_info.major==3, 'This notebook must be run using Python 3.'\n",
    "\n",
    "import tensorflow as tf\n",
    "# print('TF version: {}'.format(tf.__version__))\n",
    "import apache_beam as beam\n",
    "#print('Beam version: {}'.format(beam.__version__))\n",
    "import tensorflow_model_analysis as tfma\n",
    "# print('TFMA version: {}'.format(tfma.__version__))\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "\n",
    "# Define a simple sequential model\n",
    "def create_model():\n",
    "  model = tf.keras.Sequential([\n",
    "    keras.layers.Dense(512, activation='relu', input_shape=(2,)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(10)\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "  return model\n",
    "\n",
    "# Create a basic model instance\n",
    "model = create_model()\n",
    "\n",
    "for kfold, (train, test) in enumerate(KFold(n_splits=3, \n",
    "                                shuffle=True).split(X, y)):\n",
    "    # clear the session \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # calling the model and compile it \n",
    "    #seq_model = my_model()\n",
    "    model.compile(\n",
    "        loss  = tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics  = tf.keras.metrics.CategoricalAccuracy(),\n",
    "        optimizer = tf.keras.optimizers.Adam())\n",
    "\n",
    "    print('Train Set')\n",
    "    print(X[train].shape)\n",
    "    print(y[train].shape)\n",
    "\n",
    "    print('Test Set')\n",
    "    print(X[test].shape)\n",
    "    print(y[test].shape)\n",
    "\n",
    "    # run the model \n",
    "    model.fit(X[train], y[train],\n",
    "              batch_size=128, epochs=2, validation_data=(X[test], y[test]))\n",
    "    \n",
    "    model.save_weights(f'wg_{kfold}.h5')\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# The tf.keras.callbacks.ModelCheckpoint callback allows you to continually \n",
    "# save the model both during and at the end of training\n",
    "\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Train the model with the new callback\n",
    "model.fit(X[train], \n",
    "          y[train],  \n",
    "          epochs=10,\n",
    "          validation_data=(X[test], y[test]),\n",
    "          callbacks=[cp_callback])  # Pass callback to training\n",
    "\n",
    "filepath = \"/home/antillas/collabos/model_aggregation\"\n",
    "\n",
    "# Save all The model's configuration (architecture), The model's weights, \n",
    "# Saves a model as a .keras file - The model's optimizer's state (if any)\n",
    "\n",
    "model.save(\"kera_model_one.keras\")\n",
    "\n",
    "# Saves all layer weights to a .weights.h5 file.\n",
    "model.save_weights(\n",
    "    filepath, overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa96d0a-273f-4f54-9da7-5159329a55cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1006610/4155883061.py\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_federated\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_federated/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# the directory structure. The python import statements above implicitly add\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# these to locals().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mpython\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mproto\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "# Next steps for the \n",
    "'''\n",
    "1: Use the metadat extractor here to retrieve any metadata about the pretrained models\n",
    "\n",
    "2: A component for the metadata schema validator\n",
    "\n",
    "3: Give weights to each model metadata -> Kind of a fitnesss functions\n",
    "\n",
    "4: Encode metadata as chromosomes\n",
    "\n",
    "5: Do generic algorithm for Model aggregation\n",
    "\n",
    "6: Do FedAvg aggregations, Bagging/stacking/Voting\n",
    "\n",
    "7: Compare results from 5 to results from 6\n",
    "'''\n",
    "\n",
    "'''\n",
    "  FedAvg - Load pretrained models and do aggregations - mobilenet models\n",
    "'''\n",
    "\n",
    "\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "!pip install tf-keras==2.15.0\n",
    "!pip install tensorflow==2.15.2\n",
    "\n",
    "def model_fn():\n",
    "  # We _must_ create a new model here, and _not_ capture it from an external\n",
    "  # scope. TFF will call this within different graph contexts.\n",
    "  # Get pre-trained mobileNet models \n",
    "  keras_model = timm.create_model('mobilenetv4_hybrid_large.ix_e600_r384_in1k', pretrained=True, features_only=True,)\n",
    "    \n",
    "  # keras_model = create_keras_model()\n",
    "  return tff.learning.models.from_keras_model(\n",
    "      keras_model,\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "\n",
    "if \"__name__\" == main:\n",
    "    training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
    "        model_fn,\n",
    "        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n",
    "\n",
    "    print(training_process.initialize.type_signature.formatted_representation())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c181d55-082f-41ca-a57b-2d1c19d0cfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 9\n",
      "-->>  "
     ]
    }
   ],
   "source": [
    "# Model metadata setting up \n",
    "# And splitting dataset for drift measures\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# from scikit-image.io import imread, imshow\n",
    "\n",
    "def calculate_psi(expected, actual, buckettype='bins', buckets=10, axis=0):\n",
    "\n",
    "    def psi(expected_array, actual_array, buckets):\n",
    "        def scale_range (input, min, max):\n",
    "            input += -(np.min(input))\n",
    "            input /= np.max(input) / (max - min)\n",
    "            input += min\n",
    "            return input\n",
    "\n",
    "        breakpoints = np.arange(0, buckets + 1) / (buckets) * 100\n",
    "        breakpoints = scale_range(breakpoints, np.min(expected_array), np.max(expected_array))\n",
    "        expected_percents = np.histogram(expected_array, breakpoints)[0] / len(expected_array)\n",
    "        actual_percents = np.histogram(actual_array, breakpoints)[0] / len(actual_array)\n",
    "        def sub_psi(e_perc, a_perc):\n",
    "            if a_perc == 0:\n",
    "                a_perc = 0.0001\n",
    "            if e_perc == 0:\n",
    "                e_perc = 0.0001\n",
    "\n",
    " \n",
    "\n",
    "            value = (e_perc - a_perc) * np.log(e_perc / a_perc)\n",
    "            return(value)\n",
    "\n",
    " \n",
    "\n",
    "        psi_value = np.sum(sub_psi(expected_percents[i], actual_percents[i])\n",
    "\n",
    "                                             for i in range(0, len(expected_percents)))\n",
    "\n",
    "        return(psi_value)\n",
    "\n",
    "    if len(expected.shape) == 1:\n",
    "        psi_values = np.empty(len(expected.shape))\n",
    "    else:\n",
    "        psi_values = np.empty(expected.shape[axis])\n",
    "\n",
    "    for i in range(0, len(psi_values)):\n",
    "        psi_values = psi(expected, actual, buckets)\n",
    "\n",
    "    return(psi_values)\n",
    "\n",
    "\n",
    "# Trying a different PSI procedure\n",
    "def psi(reference, monitored, bins=None):\n",
    "    \"\"\"\n",
    "    Calculate the Population Stability Index (PSI) between a reference dataset and a monitored dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    reference (numpy.array): The reference dataset, representing the baseline distribution.\n",
    "    monitored (numpy.array): The monitored dataset, representing the distribution to compare against the reference.\n",
    "    bins (int, optional): The number of bins to use for the histograms. If set to None, Doane's formula will be used to calculate the number of bins. Default is None.\n",
    "    \n",
    "    Returns:\n",
    "    float: The calculated PSI value. A higher value indicates greater divergence between the two distributions.\n",
    "    \"\"\"\n",
    "    # Get the full dataset\n",
    "    full_dataset = np.concatenate((reference, monitored))\n",
    "\n",
    "    # If bins is not parametrized, use Doane's formula for calculating number of bins\n",
    "    if bins is None:\n",
    "        _, bin_edges = np.histogram(full_dataset, bins=\"doane\")\n",
    "    else:  # If number of bins is specified\n",
    "        bin_edges = np.linspace(min(min(reference), min(monitored)), max(max(reference), max(monitored)), bins + 1)\n",
    "\n",
    "    # Calculate the histogram for each dataset\n",
    "    reference_hist, _ = np.histogram(reference, bins=bin_edges)\n",
    "    monitored_hist, _ = np.histogram(monitored, bins=bin_edges)\n",
    "\n",
    "    # Convert histograms to proportions\n",
    "    reference_proportions = reference_hist / np.sum(reference_hist)\n",
    "    monitored_proportions = monitored_hist / np.sum(monitored_hist)\n",
    "\n",
    "    # Replace zeroes to avoid division by zero or log of zero errors\n",
    "    monitored_proportions = np.where(monitored_proportions == 0, 1e-6, monitored_proportions)\n",
    "    reference_proportions = np.where(reference_proportions == 0, 1e-6, reference_proportions)\n",
    "\n",
    "    # Calculate PSI\n",
    "    psi_values = (monitored_proportions - reference_proportions) * np.log(monitored_proportions / reference_proportions)\n",
    "    psi = np.sum(psi_values)\n",
    "\n",
    "    print(\"************* \", psi)\n",
    "\n",
    "    return psi\n",
    "\n",
    "## Calculate psi for features\n",
    "psi_list = []\n",
    "\n",
    "# top_feature_list=df_salary_high.columns\n",
    "'''\n",
    "for feature in range(len(train_images[0])): #top_feature_list:\n",
    "        # Assuming you have a validation and training set\n",
    "        #psi_t = calculate_psi( dataset_ver1[0][feature], train_images[0][feature])\n",
    "        # psi_t = psi( dataset_ver1[feature], train_images[feature])\n",
    "        psi_list.append(psi_t)      \n",
    "        print('Stability index for column ',feature,'is',psi_t)\n",
    "'''\n",
    "\n",
    "def dataset_ver_one (image):\n",
    "    return ndimage.prewitt(image, axis=0)\n",
    "    #return ds.filter(lambda x: x < 5)\n",
    "\n",
    "def dataset_ver_two (image):\n",
    "    return ndimage.prewitt(image, axis=1)\n",
    "    #return ds.filter(lambda x: x < 5)\n",
    "\n",
    "def dataset_ver_three (prewitt_h, prewitt_v):\n",
    "    magnitude = np.sqrt(prewitt_h ** 2 + prewitt_v ** 2)\n",
    "    magnitude *= 255 / np.max(magnitude)\n",
    "    return magnitude\n",
    "    # dimage.prewitt(ds[0], axis=1)\n",
    "    # return ds.filter(lambda x: x < 5)\n",
    "    \n",
    "def generate_dataset_from_mnist(train_images, train_labels):\n",
    "    new_dataset = train_images.apply(dataset_fn)\n",
    "    list(dataset.as_numpy_iterator())\n",
    "\n",
    "    \n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "# print(len(train_images), train_labels[3000])\n",
    "\n",
    "# plt.matshow(train_images[0])\n",
    "\n",
    "# edges_prewitt_vertical = prewitt_v(train_images[0])\n",
    "dataset_ver1 = []\n",
    "dataset_ver2 = []\n",
    "dataset_ver3 = []\n",
    "\n",
    "for t in range(len(train_images)):\n",
    "    prewitt_h = dataset_ver_one(train_images[t]) #, axis=0)\n",
    "    prewitt_v = dataset_ver_two(train_images[t]) #, axis=1)    \n",
    "    magnitude = dataset_ver_three(prewitt_h, prewitt_v)\n",
    "    dataset_ver1.append(prewitt_h)\n",
    "    dataset_ver2.append(prewitt_v)\n",
    "    dataset_ver3.append(magnitude)\n",
    "\n",
    "#df_1 = pd.DataFrame(dataset_ver1)\n",
    "#df_2 = pd.DataFrame(dataset_ver2)\n",
    "#df_3 = pd.DataFrame(dataset_ver3)\n",
    "#print(\"**** \", df_1)\n",
    "\n",
    "# print(\"-->> \", dataset_ver1)\n",
    "\n",
    "# print(\"***************** \", len(dataset_ver3))\n",
    "# magnitude = np.sqrt(prewitt_h ** 2 + prewitt_v ** 2)\n",
    "# magnitude *= 255 / np.max(magnitude) # Normalization\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize = (8, 8))\n",
    "\n",
    "#axes[0, 0].imshow(train_images[3000])\n",
    "#axes[0, 1].imshow(dataset_ver1[3000])\n",
    "#axes[1, 0].imshow(dataset_ver2[3000])\n",
    "#axes[1, 1].imshow(dataset_ver3[3000])\n",
    "#titles = [\"original dataset\", \"dataset_ver1\", \"dataset_ver2\", \"dataset_ver3\"]\n",
    "#for i, ax in enumerate(axes.ravel()):\n",
    "#    ax.set_title(titles[i])\n",
    "#    ax.axis(\"off\")\n",
    "\n",
    "#plt.show() \n",
    "#plt.show() \n",
    "\n",
    "\n",
    "#base_df = pd.DataFrame(train_images)\n",
    "#current_df = pd.DataFrame(dataset_ver1)\n",
    "'''\n",
    "    We try to adjust the image feature set here\n",
    "    Adding or removing some features, or adding mear gaussian noise\n",
    "\n",
    "import mahotas as mh\n",
    "def image_noiser(image_shape)->image:\n",
    "    im = mh.imread('7Esco.jpg', as_grey=1)\n",
    "    # load image and convert to gray\n",
    "    im2 = im[::2,::2]\n",
    "    im2 = mh.gaussian_filter(im2, 1.4)\n",
    "    # downsample and blur (remove noise)\n",
    "    im2 = 255 - im2\n",
    "    # invert the image\n",
    "    mean_filtered = mh.convolve(im2.astype(float), np.ones((9,9))/81.)\n",
    "    # mean filtering with a convolution\n",
    "    imc = im2 > mean_filtered - 4\n",
    "    # might need to adjust the number 4 here, but it worked well for this image.\n",
    "    mh.imsave('binarized.png', (imc*255).astype(np.uint8))\n",
    "    \n",
    "    #gauss_noise = np.zeros((640,480),dtype=np.uint8)\n",
    "    #cv2.randn(gauss_noise,128,20)\n",
    "    #gauss_noise=(gauss_noise*0.5).astype(np.uint8)\n",
    "'''\n",
    "\n",
    "def get_drift_in_dataset(base_df,current_df)->bool:\n",
    "    status = True\n",
    "    report={}\n",
    "    threshold=0.05\n",
    "    for column in base_df.columns:\n",
    "        d1 = base_df[column]\n",
    "        d2 = current_df[column]\n",
    "        is_same_dist = ks_2samp(d1,d2)\n",
    "\n",
    "        if threshold<=is_same_dist.pvalue:\n",
    "            is_found=False\n",
    "        else:\n",
    "            status = False\n",
    "            is_found=True\n",
    "    \n",
    "        report.update({column:{\"p_value\":float(is_same_dist.pvalue),\"drift_status\":is_found}}) \n",
    "    print(\"p_value: \",float(is_same_dist.pvalue),\"drift_status: \",is_found)\n",
    "    return report\n",
    "\n",
    "# for t in range(len(train_images[1])):\n",
    "#base_df =  pd.Dataframe(train_images # [t] # pd.DataFrame(t) #\n",
    "#current_df =  dataset_ver1 # [t] # pd.DataFrame(t) #\n",
    "#get_drift_in_dataset(base_df, current_df)\n",
    "\n",
    "#base_df =  pd.DataFrame(train_images[0]) # [t] # pd.DataFrame(t) #\n",
    "#current_df =  pd.DataFrame(dataset_ver1[0]) # [t] # pd.DataFrame(t) #\n",
    "#get_drift_in_dataset(base_df, current_df)\n",
    "\n",
    "\n",
    "from evidently.report import Report\n",
    "from evidently.metrics import DataDriftTable\n",
    "from evidently.metrics import DatasetDriftMetric\n",
    "\n",
    "data_drift_dataset_report = Report(metrics=[DatasetDriftMetric(), DataDriftTable(), ])\n",
    "\n",
    "data_drift_dataset_report.run(reference_data=pd.DataFrame(train_images), current_data=pd.DataFrame(dataset_ver1))\n",
    "\n",
    "data_drift_dataset_report\n",
    "\n",
    "# To speed up these runs, use the first 1000 examples\n",
    "train_labels = train_labels[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\n",
    "test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232337ab-346c-4fab-a8ad-5e25ad1e03e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
